# Configuration file for the carbon simulator

# This file is organized into sections defined in [] brackets as in [section].
# Sections may be hierarchical withsub-sections split by the '/' character as
# in [section/sub_section].
#
# values can be "strings" , numbers, or true/false, existing values
# should indicate the type

# This section controls various high-level simulation parameters.
[general]
# This is the directory where the logs as well as the simulation
# result is stored.
output_dir = "./output_files/"

# This is the name of the file, under the output directory (defined
# above) that the statistics for each core are written to.
output_file = "sim.out"

# Total number of cores in the simulation
#total_cores = 64
total_cores = 65

# Max threads per core
max_threads_per_core = 1

# This defines the number of processes that will used to
# perform the simulation
num_processes = 1

# these flags are used to disable certain sub-systems of
# the simulator and should only be used/changed for debugging
# purposes.
enable_performance_modeling = true
enable_power_modeling = false
enable_area_modeling = false
enable_shared_mem = true
enable_syscall_modeling = true

# Simulator Mode (full, lite)
mode = full

# Trigger models within application using CarbonEnableModels() and CarbonDisableModels()
trigger_models_within_application = false

# Technology Node for area and power modeling of caches and network
# McPAT works at (22,32,45,65,90,180) nm and Orion works at (32,45,65,90) nm [Take intersection]
technology_node = 65                     # In nm (Allowed values are 32,45,65,90)

# Location of McPAT installation
McPAT_home = "/path/to/McPAT"

# Width of a Tile
tile_width = 1.0  # In mm

# Enable shared memory shortcut for network models
enable_shared_memory_shortcut_for_network = false

# This option defines the ports on which the various processes will communicate
# in distributed simulations. Note that several ports will be used above this
# number for each process, thus requiring a port-range to be opened for
# distributed simulations.
[transport]
base_port = 2000

# This section is used to fine-tune the logging information. The logging may
# be disabled for performance runs or enabled for debugging.
[log]
enabled = false
stack_trace = false
disabled_modules = ""
enabled_modules = ""

[progress_trace]
enabled = false
interval = 5000

[thread_scheduling]
scheme = none                          # Valid Schemes: none, round_robin
quantum = 100

# this section defines the sychronization mechanism. For more information
# on tradeoffs between the different synchronization schemes, see the
# Graphite paper from HPCA.
[clock_skew_minimization]
scheme = lax                           # Valid Schemes are 'lax,lax_barrier,lax_p2p'

# These are the various parameters used for each synchronization scheme
# with the comments defined inline
[clock_skew_minimization/lax_barrier]
quantum = 1000                         # In ns. Synchronize after every quantum
[clock_skew_minimization/lax_p2p]
quantum = 1000                         # In ns. Could be equal to slack but kept different for generality
slack = 1000                           # In ns
sleep_fraction = 1.0                   # Equal to the fraction of computed time the core sleeps

# Since the memory is emulated to ensure correctness on distributed simulations, we
# must manage a stack for each thread. These parameters control information about
# the stacks that are managed.
[stack]
stack_base = 2415919104                # This is the start address of the managed stacks
stack_size_per_core = 2097152          # This is the size of the stack

# The process map is used for multi-machine distributed simulations. Each process
# must have a hostname associated with it and this mapping below describes the
# mapping between processes and hosts. 
[process_map]
process0 = "127.0.0.1"
process1 = "127.0.0.1"
process2 = "127.0.0.1"
process3 = "127.0.0.1"
process4 = "127.0.0.1"
process5 = "127.0.0.1"
process6 = "127.0.0.1"
process7 = "127.0.0.1"
process8 = "127.0.0.1"
process9 = "127.0.0.1"
process10 = "127.0.0.1"
process11 = "127.0.0.1"
process12 = "127.0.0.1"
process13 = "127.0.0.1"
process14 = "127.0.0.1"
process15 = "127.0.0.1"
process16 = "127.0.0.1"

# This section describes parameters for the core model
[core]
# Format: "tuple_1, tuple_2, ..., tuple_n"
#    where tuple_i = <number of cores, frequency, core type, L1-I cache configuration, L1-D cache configuration, L2 cache configuration>
# Use 'default' to accept the default values for any parameter

# Default Number of Cores = 'general/total_cores'

# Frequency is specified in GHz (floating point values accepted)
# Default Frequency = 1 GHz

# Valid core types are simple, iocoom
# Default Core Type = simple

# New configurations can be added easily
# Default cache configuration is T1

model_list = "<default,1,simple,T1,T1,T1>"

[core/iocoom]
num_store_buffer_entries = 8
num_outstanding_loads = 8

# This section describes the number of cycles for
# various arithmetic instructions.
[core/static_instruction_costs]
add=1
sub=1
mul=3
div=18
fadd=3
fsub=3
fmul=5
fdiv=6
generic=1
jmp=1

[branch_predictor]
type=one_bit
mispredict_penalty=14                     # In cycles
size=1024

# L1-I, L1-D and L2 Caches are in the same clock domain as the core
[l1_icache/T1]
cache_line_size = 64                      # In Bytes
cache_size = 32                           # In KB
associativity = 4
replacement_policy = lru
data_access_time = 1                      # In cycles
tags_access_time = 1                      # In cycles
perf_model_type = parallel
track_miss_types = true

[l1_dcache/T1]
cache_line_size = 64                      # In Bytes
cache_size = 32                           # In KB
associativity = 4
replacement_policy = lru 
data_access_time = 1                      # In cycles
tags_access_time = 1                      # In cycles
perf_model_type = parallel
track_miss_types = true

[l2_cache/T1]
cache_line_size = 64                      # In Bytes
cache_size = 512                          # In KB
associativity = 8
replacement_policy = lru                  # Not documented but I'm guessing pseudo-LRU
data_access_time = 8                      # In cycles
tags_access_time = 3                      # In cycles
perf_model_type = parallel
track_miss_types = true

[caching_protocol]
type = pr_l1_pr_l2_dram_directory_msi
# Available values are
# 1) pr_l1_pr_l2_dram_directory_msi
# 2) pr_l1_pr_l2_dram_directory_mosi
# 3) sh_l1_sh_l2
# 4) pr_l1_sh_l2_msi
unmodeled_miss_types = ""
# Use a comma-separated list consisting of [cold, capacity, sharing]
# Only works for pr_l1_pr_l2_dram_directory_msi and pr_l1_pr_l2_dram_directory_mosi protocols

[caching_protocol/pr_l1_pr_l2_dram_directory_mosi]
switch_networks = false

[caching_protocol/pr_l1_sh_l2_msi]
switch_networks = false

[l2_directory]
max_hw_sharers = 64                       # number of sharers supported in hardware (ignored if directory_type = full_map)
directory_type = full_map                 # Supported (full_map, limited_broadcast, limited_no_broadcast, ackwise, limitless)
access_time = 8                           # In cycles

[dram_directory]
total_entries = 16384
associativity = 16
max_hw_sharers = 64                       # number of sharers supported in hardware (ignored if directory_type = full_map)
directory_type = full_map                 # Supported (full_map, limited_broadcast, limited_no_broadcast, ackwise, limitless)
access_time = 8                           # In cycles

[limitless]
software_trap_penalty = 200
# number of cycles added to clock when trapping into software 
# (pulled number from Chaiken papers, which explores 25-150 cycle penalties)

[dram]
latency = 100                             # In ns
per_controller_bandwidth = 5              # In GB/s
num_controllers = ALL                     
# "ALL" denotes that a memory controller is present on every tile(/core). Set num_controllers to a numeric value less than or equal to the number of cores
controller_positions = ""
[dram/queue_model]
enabled = true
type = history_tree

[power_model/dram]
dynamic_energy = 6e-10                    # In J per access
static_power = 6e-2                       # In W

# This describes the various models used for the different networks on the core
[network]
# Valid Networks : 
# 1) magic 
# 2) emesh_hop_counter, emesh_hop_by_hop
# 3) atac
#user_model_1 = emesh_hop_counter
#user_model_2 = emesh_hop_counter
#memory_model_1 = emesh_hop_counter
#memory_model_2 = emesh_hop_counter
#system_model = magic

user_model_1 = magic
user_model_2 = magic
memory_model_1 = magic
memory_model_2 = magic
system_model = magic

# emesh_hop_counter (Electrical Mesh Network)
#  - No contention models
#  - Just models hop latency and serialization latency
[network/emesh_hop_counter]
frequency = 1                    # In GHz
flit_width = 64                  # In bits
[network/emesh_hop_counter/router]
delay = 1                        # In cycles
num_flits_per_port_buffer = 4    # Number of flits per output buffer per port
[network/emesh_hop_counter/link]
delay = 1                        # In cycles
type = electrical_repeated

# emesh_hop_by_hop (Electrical Mesh Network)
#  - Link Contention Models present
#  - Infinite Output Buffering (Finite Output Buffers assumed for power modeling)
[network/emesh_hop_by_hop]
frequency = 1                    # In GHz
flit_width = 64                  # In bits
broadcast_tree_enabled = true    # Is broadcast tree enabled?
[network/emesh_hop_by_hop/router]
delay = 1                        # In cycles
num_flits_per_port_buffer = 4    # Number of flits per output buffer per port
[network/emesh_hop_by_hop/link]
delay = 1                        # In cycles
type = electrical_repeated
[network/emesh_hop_by_hop/queue_model]
enabled = true
type = history_tree

# atac_cluster (ATAC network model)
[network/atac]
frequency = 1
flit_width = 64
cluster_size = 4                             # Number of cores per cluster
laser_modes = "idle,unicast,broadcast"       # Comma separated list containing one or more of [idle,unicast,broadcast]
receive_network_type = star                  # [htree, star]
num_receive_networks_per_cluster = 2         # Number of receive networks per cluster
num_optical_access_points_per_cluster = 4    # Number of Optical Access Points per cluster
global_routing_strategy = cluster_based      # [cluster_based, distance_based]
unicast_distance_threshold = 4               # Distance above which unicasts are sent on ONet
electrical_link_type = electrical_repeated   # electrical_repeated

[network/atac/enet]
[network/atac/enet/router]
delay = 1                        # In cycles (ENet is modeled similar to an electrical mesh now) 
num_flits_per_port_buffer = 4    # Number of Buffer flits per port (Output Buffering assumed for power modeling)

[network/atac/onet]
[network/atac/onet/send_hub]
[network/atac/onet/send_hub/router]
delay = 1                        # In cycles 
num_flits_per_port_buffer = 4    # Number of Buffer flits per port (Output Buffering assumed for power modeling)

[network/atac/onet/receive_hub]
[network/atac/onet/receive_hub/router]
delay = 1                        # In cycles 
num_flits_per_port_buffer = 4    # Number of Buffer flits per port (Output Buffering assumed for power modeling)

[network/atac/star_net]
[network/atac/star_net/router]
delay = 1                        # In cycles 
num_flits_per_port_buffer = 4    # Number of Buffer flits per port (Output Buffering assumed for power modeling)

[network/atac/queue_model]
enabled = true
type = history_tree

[atac/design]
lambda_queue_length = 100
vc_buffer_size = 100
delay_to_access_queue = 3
delay_optical_network = 2

# Queue Models
[queue_model/basic]
moving_avg_enabled = true
moving_avg_window_size = 64
moving_avg_type = arithmetic_mean

[queue_model/history_list]
# Uses the analytical model (if enabled) to calculate delay
# if cannot be calculated using the history list
max_list_size = 100
analytical_model_enabled = true
interleaving_enabled = true 

[queue_model/history_tree]
# Uses the analytical model (if enabled) to calculate delay
# if cannot be calculated using the history tree
max_list_size = 100
analytical_model_enabled = true

# Collect time-varying statistics from the simulator
# For tracing to be done
#  (1) Set [statistics_trace/enabled] = true
#  (2) Set the statistics that you want to measure periodically in [statistics_trace/statistics]
#  (3) Use the barrier synchronization model (set [clock_skew_minimization/scheme] = barrier)
#  (4) Use a sampling interval >= [clock_skew_minimization/barrier/quantum] and a multiple of it
[statistics_trace]
enabled = false
statistics = "cache_line_replication, network_utilization"
# Comma separated list of statistics for which tracing is done when enabled.
# Choose from [cache_line_replication, network_utilization]
sampling_interval = 10000
# Interval between successive samples of the trace (in ns)
[statistics_trace/network_utilization]
enabled_networks = "memory_1"
# Comma separated list of networks for which injection rate is traced if enabled
# Choose from [user_1, user_2, memory_1, memory_2, system]

# Optical Link Model
[link_model/optical]
[link_model/optical/delay]
waveguide_delay_per_mm = 10e-3      # In nanoseconds
E-O_conversion = 1                  # In cycles
O-E_conversion = 1                  # In cycles
[link_model/optical/power]
ring_tuning_power = 0.16e-3         # In watts
laser_power_per_receiver = 0.015e-3 # In watts
[link_model/optical/power/static]
electrical_tx_power = 50e-6         # In watts
electrical_rx_power = 10e-6         # In watts
[link_model/optical/power/dynamic]
electrical_tx_energy = 100e-15      # In joules
electrical_rx_energy = 50e-15       # In joules
